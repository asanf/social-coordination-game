#!/usr/bin/python
# -*- coding: utf-8 -*-
'''
    Implementation of social coordination games with discreet preferences.
    
    Copyright (C) 2014  Antonio Sanfelice

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    @package Social Coordination Game    
    Social Coordination Game
    
    This code implements social coordination games with discreet preferences, 
    based on the model described by E. Anshelevich and S. Sekar in their paper:
    "Approximate Equilibrium and Incentivizing Social Coordination" which can
    be found here: http://arxiv.org/abs/1404.4718
    
    Briefly, each player has to choose among a set of choices, obtaining a 
    payoff depending on her preference towards the chosen strategy and number
    of her neighbours who made the same choice. Then the payoff is
    
    u_i(s) = w_{i}^{s} + \sum_{j \in N(i): s_i = s_j} g_ij * w_ij
    where w_ij is the payoff generated by the coordination between i and j,
    and g_ij indicates the share of w_ij obtained by i. It follows that
    g_ij + g_ji = 1
    
    The objective is to find a strategy profile (an association player/strategy)
    which is an equilibrium (no player would deviate as she is already gaining 
    the maximum payoff given the situation) and that maximizes the social welfare 
    (i.e. the sum of payoffs of all players)
    
'''
from __future__ import division
import networkx as nx
import random as rnd
import sys
import time
import cPickle as Pickle
import csv
import multiprocessing as mp

'''
    Counter needed to keep count of progress during parallel execution,
    and relative lock
'''
counter = mp.Value('i',0)
countLock = mp.Lock()



def sum(addends, start=0):
    '''
        Numerically stable function that computes the sum of a collection of values.
        Implements Kahan algorithm.
        @param addends list of value to sum
        @param start initial value
    '''
    s = start
    comp = 0.0

    for a in addends:
        y = a - comp
        t = s + y
        comp = (t - s) - y
        s = t
    return s


def stats(collection, suffix=None):
    '''
        Numerically stable function which computes mean, variance, min and max
        @param suffix a string to append to the returned dict's keys.
        @returns a dictionary with mean, variance, min and max
    '''
    n = 0
    mean = 0
    M2 = 0
    vmin = 10
    vmax = 0

    for i in collection:
        if i < vmin:
            vmin = i
        if i > vmax:
            vmax = i
        n += 1
        delta = i - mean
        mean = mean + delta/n
        M2 = M2 + delta * (i - mean)
    var = M2/(n-1)
    toReturn = dict()
    toReturn["min" + suffix] = vmin
    toReturn["max" + suffix] = vmax
    toReturn["avg" + suffix] = mean
    toReturn["var" + suffix] = var
    return toReturn

def printResults(results, filt=None):
    pass


def loadCSVResults(fileName):
    '''
        Function which loads data from a CSV file.
    '''
    headers = None
    results = []
    with open(fileName, 'rb') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            if reader.line_num == 1:
                headers = row[0:]
            else:
                newRow = dict(zip(headers, row))
                for field in headers:
                    # try to convert to float, in case of errors leave as it is
                    try:
                        newRow[field] = float(newRow[field])
                    except Exception:
                        pass

                results.append(newRow)
    return results


def saveCSVResults(results, prefix="results_"):
    '''
        Function which saves results in a timestamped CSV file.
        @param prefix a string to prepend to the resulting filename
    '''
    keys = results[0].keys()
    t = str(time.strftime("%a_%b_%d_%H%M%S"))
    f = open( prefix + t +'.csv','wb')
    dict_writer = csv.DictWriter(f, keys)
    dict_writer.writer.writerow(keys)
    dict_writer.writerows(results)
    f.close()
    return results

def saveResults(results, prefix="results"):
    t = str(time.strftime("%a_%b_%d_%H%M%S"))
    f = open(prefix + '_'+ t +'.pkl','wb')
    pickler = Pickle.Pickler(f, -1)
    pickler.dump(results)
    f.close()
    return results

def loadResults(fileName):
    f = open(fileName, 'rb')
    results = Pickle.load(f)
    f.close()
    return results

# custom exception
class SCGException(Exception):
    pass


class Game:
    '''
        Class modelling an istance of a social coordination game with discreete
        preferences.
    
        @param numPlayers the number of players in the game (the nodes of the graph)
        @param numStrategies the number of choices per player
        @param completeness a value in (0,1] which indicates how much the graph
                        should be close to a complete graph. If None it will be
                        chosen randomly.
    '''
    def __init__(self, numPlayers, numStrategies, completeness=None):
        
        if numPlayers < 3:
            raise SCGException("Less than 3 players does not make much sense here")
        
        if numStrategies < 2:
            raise SCGException("1 strategy? You're not leaveing any choice, are you?")
        
        self.numStrategies = numStrategies
        self.numPlayers = numPlayers
        maxPayoff = 100

        # minimum number of edges
        a = numPlayers - 1

        # maximum number of edges (complete graph)
        b = a * numPlayers / 2

        # choose the number of edges accordingly to the completeness value or 
        # randomly if None
        if completeness is None:
            numEdges = rnd.randint(a,b)
        elif 0 < completeness <= 1:
            numEdges = int(completeness * b)
        else:
            raise ValueError("Completeness not in (0,1]")

        # create an empty graph
        self.graph = nx.Graph()

        '''
        Create a list of nodes (players) and add it to the graph
            gamma: the interest towards neighbours choises
            share: percentage of the edge payoff obtained upon coordination
            belief: list of preference of the player towards each strategy
            strategy: strategy played by the player
        '''
        nodi = [(i, {"gamma":rnd.random(),
                     "share": dict(),
                     "belief":[0] * numStrategies,
                     "strategy":0,
                     "influence":0}) for i in range(numPlayers)]
        self.graph.add_nodes_from(nodi)

        '''
            Get the list of nodes. (previous list wouldnt work as it is
            a list of dictionaries, not a list of ints)
        '''
        nodi = list(self.graph.nodes())


        '''
        Generate a connected graph. The graph needs to be connected since 
        isolated vertices or connected components are of no interest.
        Isolated nodes would end up playing their preferred strategy, while
        connected components can be seen as distinct istances of the problem.
        
        To assure that the graph is connected, there must be at least an
        incident edge for every node.
        '''

        # for every node except the first
        for u in nodi[1:]:

            # choose a node randomly from the previous nodes
            v = rnd.choice(nodi[:u])

            # create an edge between u and v with weight (w_ij) chosen at random
            # the range starts with 1 as a 0 weighted edge would mean no gain
            # for collaboration between the two players, which is the same as
            # no edge between the two
            self.graph.add_edge(v, u, weight = rnd.uniform(1,maxPayoff))

            # decrement the number of edges to create
            numEdges = numEdges - 1

        '''
        At the end of the previous step, a connected graph is assured with
        exactly n-1 edges. The following step will add the remaining edges,
        which are numEdges - n + 1
        '''

        # while there are edges to add
        while numEdges > 0:

            # choose two node randomly
            u, v = rnd.sample(nodi, 2)

            # if (u, v) does not already exist
            if not self.graph.has_edge(u, v):

                # create the edge and decrement the counter
                self.graph.add_edge(u, v, weight = rnd.uniform(1, maxPayoff))
                numEdges = numEdges - 1

        '''
        Now that the graph is created, the parameters for the single node must
        be initializated. Players egoistic payoffs (beliefs) towards each 
        strategy must be set while trying to vary the ratio egoistic payoff over 
        coordination payoff, as if the ratio is less than 1 for every node,
        everyone would coordinate, else (>1) no one would.
        '''

        # for every players and relative attributes
        for p, pAttr in self.graph.nodes(data = True):
            # coord will cotain each coordination payoff
            coord = []
            
            # for every node in p neighbourhood
            for n in self.graph.neighbors(p):
                
                # if gamma_ij has not been computed, compute it
                if not pAttr['share'].has_key(n):
                    i = pAttr['gamma']
                    j = self.graph.node[n]['gamma']
                    pAttr['share'][n] = i / (i+j)
                    self.graph.node[n]['share'][p] = 1 - pAttr['share'][n]

                coord.append(pAttr['share'][n] * self.graph[p][n]['weight'])
            
            '''
            chose at randomly how far should the belief be from the coordination
            0: the player search for pure coordination, has no personal interest
            1: the player is indifferent between coordination and personal preference
            >1: the player has a dominant strategy, which is playing the strategy
            she prefers
            '''
            ego = rnd.uniform(0, 2)
            maxBelief = sum(coord) * ego
            
            # compute the preferences for each strategy
            pAttr['belief'] = [rnd.uniform(0, maxBelief) for s in range(numStrategies)]


    def normalize(self):
        '''
            Method which normalizes all the payoffs in the [0,1] range
        '''
        weights = [self.w(i,j) for i,j in self.graph.edges()]
        for i in self.players():
            weights += self.belief(i)

        minWeight = min(weights)
        maxWeight = max(weights)


        denom = maxWeight - minWeight

        norm = lambda x: (x - minWeight)/denom

        for p in self.players():
            self.graph.node[p]['belief'] = map(norm, self.belief(p))

        for i, j in self.graph.edges():
            self.graph[i][j]['weight'] = norm(self.w(i, j))

    
    def players(self):
        '''
            @returns the list of players
        '''
        return self.graph.nodes()


    def s(self, i):
        '''
        
            @returns the strategy played by player i
        '''
        return self.graph.node[i]['strategy']

    def setStrategy(self, player, strategy):
        '''
            Method which assign a strategy to a player
            @param player the player to whom assign a strategy
            @param strategy the strategy to assign
        '''

        self.graph.node[player]['strategy'] = strategy

    
    def getProfile(self):
        '''
            @return the list of strategies played by each players
        '''
        return [self.s(i) for i in self.players()]


    def setProfile(self, s):
        '''
            Method which assign a strategy to each player
            @param s a list of strategy of lenght numPlayers
            @raise SCGException if profile lenght differs from number of players
        '''
        if len(s) < self.numPlayers:
            raise SCGException("profile length less than number of players")
            
        for player, strategy in zip(self.players(), s):
            self.setStrategy(player, strategy)

    def gamma(self, i,j):
        '''
            @return the share that i obtains from by coordinating with j
        '''
        return self.graph.node[i]['share'][j]

    def w(self, i, j):
        '''
            @return the payoff generated when i and j coordinate
        '''
        return self.graph[i][j]['weight']

    def belief(self, i):
        '''
            @return the list of egoistic preference of i towards each strategy
        '''
        return self.graph.node[i]['belief']

    def best(self, i):
        '''
            @returns the strategy preferred by i
        '''
        b = self.belief(i)
        return b.index(max(b))

    def A(self):
        '''
            @return the part of the social welfare due to egoistic payoffs
        '''
        toAdd = [self.belief(i)[self.s(i)] for i in self.players()]
        return sum(toAdd)

    def P(self):
        '''
            @return the part of the social welfare due to coordination
        '''
        toAdd = [self.w(i, j) for i, j in self.graph.edges() if self.s(i) == self.s(j)]
        return sum(toAdd)

    def u(self, i, s):
        '''
            Method that compute the payoff of a player given a strategy
            @param i the player whose payoff must be computed
            @param s the strategy to examine
            @return the payoff of i given s (i.e. u_i(s))
        '''
        toAdd = [self.gamma(i, j) * self.w(i, j) for j in self.graph.neighbors(i) \
        if s == self.s(j)]
        
        # return belief + coordination
        return sum(toAdd, self.belief(i)[s])

    def swc(self, i, s):
        '''
            Method that computes the contribution of i to the social welfare
            given a strategy
            @param i the player whose contribution must be computed
            @param s the strategy to examine
            @return the contribution of i to the social welfare given s
        '''
        toAdd = [self.w(i, j) for j in self.graph.neighbors(i) \
                if s == self.s(j)]
        return sum(toAdd, self.belief(i)[s])


    def socialWelfare(self, profile = None):
        '''
            Method which computes the social welfare
            @param profile a profile to measure
            @return social welfare of the given profile or the current profile
        '''
        
        # if a profile has not been given, get the current profile
        if not profile:
            profile = self.getProfile()
            
        toAdd = [self.belief(i)[profile[i]] for i in self.players()]
        toAdd += [self.w(i,j) for i,j in self.graph.edges() if profile[i] == profile[j]]
        sw = sum(toAdd)
        del toAdd
        return sw

    def bestInitialStrategy(self):
        '''
            Method that find the best intial strategy for di one-shot alpha-BR
            algorithm. That is, amongst the profiles where all players play
            the same strategy, choose the one which maximizes the sum of the
            egoistic payoffs
        '''
        # lambda function that computes the sum of egoistic payoff for a given
        # strategy
        As = lambda s: sum([self.belief(i)[s] for i in self.players()])
        
        # compute As for every strategy
        A = [As(s) for s in range(self.numStrategies)]
        
        # return the maximizing strategy
        return A.index(max(A))
    

    def computeInfluence(self):
        '''
            Method which computes the influence of each note by the means of a
            voting system. Each player gives a mark to each of her neightbour.
            Every vote is weighted by the degree (number of neighbours) of the
            player, in order to give more importance to the opinion of those who 
            have more neighbours.
        '''
        # retrieve the degree of the nodes and normalizes them
        weight = [self.graph.degree(p) for p in self.players()]
        maxDeg = max(weight)
        weight = map(lambda x: x/maxDeg, weight)
        # gather the coordination payoff for every player
        coords = [sum([self.gamma(i,n) * self.w(i,n) for n in self.graph.neighbors(i)]) for i in self.players()]
        
        # lambda function which computes the influence of i according to j
        score = lambda i,j: (self.gamma(j,i) * self.w(j,i))/coords[j]

        # for every player
        for i in self.players():
            # influence is the sum of the score given by each of her neighbours
            influence = sum([score(i,j)*weight[j] for j in self.graph.neighbors(i)])
            self.graph.node[i]['influence'] = influence

    def computeEigenCentrality(self):
        ''' 
            Method which computes the priority of every player based
            on the eigenvalues of the adjacency matrix. First, attempt to 
            compute the pagerank, should it fail, an easier algorithm is used.
        '''
        try:
            centrality = nx.pagerank(self.graph, max_iter=1000)
        except nx.NetworkXError:
            centrality = nx.algorithms.eigenvector_centrality(self.graph)
        for p in self.players():
            self.graph.node[p]['centrality'] = centrality[p]
        del centrality
        
    '''
        The following are some priority-reletad methods     
    '''

    def checkForTie(self, priorityFunction):
        '''
            Method which check if a give priority function yields ties
        '''
        # compute the values of the priority function for each player
        values = [priorityFunction(p) for p in self.players()]
        
        # check if the number of occurence of each value is greater than one
        # at least once
        tie = True in [values.count(i) > 1 for i in values]
        return tie
    
    def eigenCentrality(self, p):
        '''
        Method which return the pagerank / eigen centrality
        as a priority score
        @param p a player index
        @return the priority of p
        '''
        return self.graph.node[p]['centrality']

    def priorityEgo(self, p):
        '''
        Computes the priority of a player as the ratio between the egoistic 
        payoff of the preferred strategy and the total coordination payoff
        @param p a player index
        @return the priority of p
        '''
        coord = sum([self.gamma(p,j) * self.w(p,j) for j in self.graph.neighbors(p)])
        return self.belief(p)[self.best(p)] / coord

    def priorityLocalEgo(self, p):
        '''
        Computes the priority of a player as the ratio between the egoistic and 
        coordination payoffs of the currently played strategy
        @param p a player index
        @return the priority of p
        '''
        coord = sum([self.gamma(p,j) * self.w(p,j) for j in self.graph.neighbors(p) if self.s(p) == self.s(j)])
        try:
            toRet = self.belief(p)[self.s(p)] / (coord)
        except ZeroDivisionError:
            toRet = float("Inf")
        return toRet

    def priorityCoordGain(self, p):
        '''
        Computes the priority of the player based on the coordination payoff
        @param p a player index
        @return the priority of p
        '''
        coord = sum([self.gamma(p,j) * self.w(p,j) for j in self.graph.neighbors(p) if self.s(p) == self.s(j)])
        return 1/(1+coord)

    def priorityWeightedEgo(self, p):
        '''
        Same as priorityEgo but weighted by the player's degree
        @param p a player index
        @return the priority of p
        '''
        return self.graph.degree(p) * self.priorityEgo(p)

    def priorityWeightedLocalEgo(self, p):
        '''
        Same as localEgo but weighted by the player's degree
        @param p a player index
        @return the priority of psave
        '''
        return self.graph.degree(p) * self.priorityLocalEgo(p)

    def priorityDelta(self, p):
        '''
        Prioritize the player according to the number of neighbours
        @param p a player index
        @return the priority of p
        '''
        return self.graph.degree(p)

    def priorityDeltaEgo(self, p):
        '''
        Same as priorityEgo, but weighted by the inverse degree
        @param p a player index
        @return the priority of p
        '''
        coord = sum([self.gamma(p,j) * self.w(p,j) for j in self.graph.neighbors(p)])
        return self.belief(p)[self.best(p)] / (self.graph.degree(p) * coord)

    def priorityDeltaLocalEgo(self, p):
        '''
        Same as priorityLocalEgo, but weighted by the inverse degree
        @param p a player index
        @return the priority of p
        '''
        coord = sum([self.gamma(p,j) * self.w(p,j) for j in self.graph.neighbors(p) if self.s(p) == self.s(j)])
        return self.belief(p)[self.s(p)] / (self.graph.degree(p) * coord)

    def priorityInfluence(self, p):
        '''
        Return the priority value computed by computeInfluence
        @param p a player index
        @return the priority of p
        '''
        return self.graph.node[p]['influence']

    
    def priorityBestBR(self, p):
        '''
        Prioritize the player according to their best response
        @param p a player index
        @return the priority of p
        '''
        maxPayoff = 0
        for s in range(self.numStrategies):
            payoff = self.u(p, s)
            if payoff > maxPayoff:
                maxPayoff = payoff
        return maxPayoff

    def priorityBestAsocialBR(self, p):
        '''
        Prioritize the player who have high best response and few neighbours
        @param p a player index
        @return the priority of p
        '''
        return self.priorityBestBR(p) / self.graph.degree(p)

    def priorityBestBRG(self, p):
        '''
        Prioritize the player according to the gain obtained by performing a 
        best response
        @param p a player index
        @return the priority of p
        '''
        return (self.priorityBestBR(p) - self.u(p, self.s(p)))

    def priorityBestAsocialBRG(self, p):
        '''
        Prioritize players who have high gain and few neighbours
        @param p a player index
        @return the priority of p
        '''
        return self.priorityBestBRG(p) / self.graph.degree(p)

    def priorityBestSWC(self, p):
        '''
        Prioritize the player according to their contribution to social welfare
        @param p a player index
        @return the priority of p
        '''
        maxCont = 0
        for s in range(self.numStrategies):
            swc = self.swc(p,s)
            if swc > maxCont:
                maxCont = swc
        return maxCont

    def priorityBestAsocialSWC(self, p):
        '''
        Prioritize players with high social welfare contribution and low degree
        @param p a player index
        @return the priority of p
        '''
        return self.priorityBestSWC(p) / self.graph.degree(p)

    def priorityBestSWCG(self, p):
        '''
        Prioritize the player according to the gain in social welfare they
        produce by performing a best response
        @param p a player index
        @return the priority of p
        '''
        return (self.priorityBestSWC(p) - self.swc(p, self.s(p)))

    def priorityBestAsocialSWCG(self, p):
        '''
        Same as priorityBestSWCG, but prioritize players with low degree
        @param p a player index
        @return the priority of p
        '''
        return self.priorityBestSWCG(p) / self.graph.degree(p)

    def priorityCenralSWC(self, p):
        '''
        Prioritize players according to their social welfare contribution 
        weighted by their pagerank/eigencentrality value
        @param p a player index
        @return the priority of p
        '''
        return (self.priorityBestSWC(p) * self.eigenCentrality(p))
    

    def egoism(self):
        '''
        Return satistics about the best egoistic/total coordination payoff ratio
        '''
        egoList = [self.priorityEgo(p) for p in self.players()]

        toReturn = stats(egoList, "Ego")
        del egoList
        return toReturn

    def localEgoism(self):
        '''
        Return statistics about the egoistic/coordination ratio
        of the currently played strategy
        '''
        egoList = [self.priorityLocalEgo(p) for p in self.players()]

        toReturn = stats(egoList, "localEgo")
        del egoList
        return toReturn

    def degree(self):
        '''
        Return statistics about the node degrees
        '''
        deg = self.graph.degree().values()
        toReturn = stats(deg, "Deg")
        del deg
        return toReturn

    def completeness(self):
        '''
        @returns a number in [0,1] which indicates "how much" the graph is
        a complete graph
        '''
        n = self.numPlayers
        return (2*self.graph.number_of_edges())/(n*(n-1))

    def gammaStats(self):
        '''
        Return statistics about the importance players give to coordination
        '''
        g = lambda p: self.graph.node[p]['gamma']

        gammas = [self.gamma(i,j)/self.gamma(j,i) for i,j in self.graph.edges()]
        gamma = max(gammas)
        del gammas
        gammas = [g(p) for p in self.players()]
        toReturn = stats(gammas,"Gamma")
        toReturn["asymmetry"] = gamma
        return toReturn

    def alphaBR(self, p, alpha):
        '''
            Method which find an alpha best-response: that is, a strategy
            which gives a gain greater than alpha in respect to the current
            strategy
        '''
        # compute the payoff for every strategy
        payoffs = [self.u(p, s) for s in range(self.numStrategies)]
        
        # the candidate best response is the one with the max payoff
        bestResponse = payoffs.index( max(payoffs) )
        
        # default return value is none
        toRet = None
        
        # if the best response assures a gain greater than alpha
        if payoffs[bestResponse] >= alpha * payoffs[self.s(p)]:
            # return value = bestResponse
            toRet = bestResponse
        del payoffs
        return toRet

    def oneShotAlphaBR(self, alpha, k0, priority = None, reverse=True):
        '''
            Base algorithm described by Ashelevich and Sekar.
            Allows each player to perform a best response only once.
            @param alpha minimum gain required for the best response
            @param k0 initial strategy
            @param priority a prioirty function
            @param reverse boolean indicating wether players should be ordered
                           in decreasing order of priority
        '''
        
        # assign the initial strategy to each player
        self.setProfile([k0] * self.numPlayers)
        
        # copy the list of players, since the algorithms needs to remove
        # players during excecution
        players = list(self.players())
        didBestResponse = True

        # while there are players and there has been a best response
        while ( players and didBestResponse ):
            # assume there won't be best responses
            didBestResponse = False

            # if a priority function has been passed, sort the players
            if priority:
                players = sorted(players, key=priority, reverse=reverse)

            # for every player
            for p in players:
                
                # find an alpha best response
                bestResponse = self.alphaBR(p, alpha)
                
                # if best response is not None
                if bestResponse:    
                    # a best response has been done
                    didBestResponse = True
                    
                    # assign the best response to the player
                    self.setStrategy(p, bestResponse)
                    
                    # remove the player from the list, so she can't do best
                    # response anymore
                    players.remove(p)
                    # a best response has been done, break the for loop
                    # and eventually update players priority
                    break


    def findEquilibria(self, alpha, priority = None, reverse=True, prefix='original'):
        '''
            Main algorithm described by Anshelevich and Sekar.
            Return the best result between one shot alpha-BR launched with 
            parameter alpha and 1/(alpha-1).
            @param alpha the minimum gain to deviate from current strategy
            @param priority a priority function
            @param reverse wether the players should be sorted in reverse order
            @param prefix a string to prepend to the returned dict's keys
            @return a dictionary with the alpha value and relative social welfare
                    of the best solution
        '''
        # retrieve the best initial strategy
        k0 = self.bestInitialStrategy()

        # run one shot alpha-br with parameter 
        self.oneShotAlphaBR(alpha, k0, priority, reverse)

        # get the social welfare of the current solution
        alphaSw = self.socialWelfare()

        if alpha > 1:
            # compute the alpha value for the next oneshot invocation
            beta = 1 / (alpha-1)
    
            # find a solution with 1/(alpha-1)
            self.oneShotAlphaBR(beta, k0, priority, reverse)
    
            # get the social welfare of the current solution
            betaSw = self.socialWelfare()
        else:
            betaSw = -1
        # put the best solution's alpha and social welfare  in a dictionary and 
        # return the results.
        result = dict()

        prefix = prefix + "_"

        if alphaSw >= betaSw:
            result[prefix + 'alpha'] = alpha
            #result[prefix + 'profile'] = prAlpha
            result[prefix + 'sw'] = alphaSw
        else:
            result[prefix + 'alpha'] = beta
            #result[prefix + 'profile'] = self.getProfile()
            result[prefix + 'sw'] = betaSw
        #result['heuristic'] = heuristic
        #result['egoistfirst'] = egoist
        return result


def test(number):
    '''
        Function which tests several priority functions
        @returns a dictionary with several data of the tested functions,
                 mainly the ratio between the results obtained by sorting the
                 players and the solution without sorting (i.e. the original
                 algorithm)
    '''
    n = rnd.randint(10, 150)
    m = rnd.randint(4, n)

    game = Game(n, m)
    game.normalize()
    game.computeInfluence()
    game.computeEigenCentrality()

    heuristics = dict(\
    influence=game.priorityInfluence, \
    ego = game.priorityEgo, \
    localEgo = game.priorityLocalEgo, \
    delta = game.priorityDelta, \
    deltaEgo = game.priorityDeltaEgo, \
    deltaLocalEgo = game.priorityDeltaLocalEgo, \
    weightedEgo = game.priorityWeightedEgo, \
    weightedLocalEgo = game.priorityWeightedLocalEgo, \
    coordGain = game.priorityCoordGain,\
    bestBR = game.priorityBestBR,\
    bestBRG = game.priorityBestBRG,\
    bestABR = game.priorityBestAsocialBR,\
    bestABRG = game.priorityBestAsocialBRG,\
    bestSWC = game.priorityBestSWC,\
    bestSWCG = game.priorityBestSWCG,\
    bestASWC = game.priorityBestAsocialSWC,\
    bestASWCG = game.priorityBestAsocialSWCG,\
    eigCent = game.eigenCentrality,\
    eigSWC = game.priorityCenralSWC
    )

    # create the result dictionary
    results = dict(players=game.numPlayers, strategies = game.numStrategies)
    
    # give the experiment an id based on current time
    results["id"] = time.strftime("%a_%b_%d_%H_%M_%S")
    
    # store the ratio between number of strategy and number of players
    results["mnratio"] = game.numStrategies / game.numPlayers

    # find the best initial strategy, and set it to all players
    k0 = game.bestInitialStrategy()
    game.setProfile([k0]*n)
    
    # get the social welfare of the best initial strategy
    initSw = game.socialWelfare()
    
    # computes the golden ratio
    phi = (1 + 5**.5)/2

    alpha = rnd.uniform(phi, 2)

    # get the alpha value and the social welfare of the original algorithm
    origResult = game.findEquilibria(alpha)
    
    # add it to the result dictionary
    results.update(origResult)
    
    # store the ratio between the solution find by the algorithm and the 
    # starting solution
    results["originalGain"] = results['original_sw'] / initSw
    best = "original"
    best_sw = results['original_sw']

    # for each priority function (prefix holds the key of the dictionary
    # relative to the function)
    for prefix, function in heuristics.iteritems():
        
        # check whether the priority fuction yields ties
        tie = game.checkForTie(function)        
        results[prefix+"Tie"] = tie

        # compute the solution prioritizing the players according to function
        # and add it to the result dictionary
        result = game.findEquilibria(alpha, priority=function, prefix=prefix)
        results.update(result)
        
        # store the gain relatively to the original algorithm
        results[prefix + "Gain"] = result[prefix+"_sw"] / results["original_sw"]
        
        # check if this is the best solution so far
        if result[prefix + '_sw'] >= best_sw:
            best_sw = result[prefix+'_sw']
            best = prefix


    results['best'] = best
    results['bestGain'] = results[best+'Gain']

    # add some statistics to the result dictionary
    results.update(game.egoism())
    results.update(game.localEgoism())
    results.update(game.degree())
    results.update(game.gammaStats())
    results["complete"] = game.completeness()

    with countLock:
        counter.value += 1

    game = None

    return results



def run(numExperiments, function=test):
    '''
    Function which execute a number of test in parallel
    @param numExperiment how many execution must be done
    @param function which function must be excecuted
    @returns a list of dictionaries of length numExperiments
    '''
    
    # create a multiprocessing pool, the number of processors is automatically
    # retrieved. maxtaskperchild puts a limit on the memory used
    pool = mp.Pool(maxtasksperchild=2500)

    # results is a list of results given by each function run.
    results = pool.map_async(function, range(numExperiments))

    startTime = time.time()

    # until the results are ready
    while not results.ready():
        # comput how much time has passed since the start of the test
        elapsed = time.time() - startTime
        hours = int(elapsed / 3600)
        minutes = int(elapsed /60)%60
        seconds = int(elapsed % 60)
        time.sleep(.5)
        
        # print progress
        done = "\rProgress: {0:3.2f} % \tRunning since: {1:02d}:{2:02d}:{3:02d}".format((counter.value/numExperiments)*100, hours, minutes, seconds)
        sys.stderr.write(done)
    
    # clear and close the pool
    pool.close()
    pool.join()
    print "\n"
    
    # returns the results in output
    output = results.get()
    return output
